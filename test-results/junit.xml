<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="2" skipped="0" tests="9" time="1.650" timestamp="2024-11-27T10:49:52.461942-05:00" hostname="xiong-desktop"><testcase classname="tests.test_chat_interface" name="test_chat_interface_initialization" time="0.001" /><testcase classname="tests.test_chat_interface" name="test_chat_interface_custom_initialization" time="0.001" /><testcase classname="tests.test_chat_interface" name="test_send_message" time="0.420"><failure message="openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: test-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}">chat_interface = &lt;streamlitchat.chat_interface.ChatInterface object at 0x000001FF59D87BF0&gt;

    @pytest.mark.asyncio
    async def test_send_message(chat_interface: ChatInterface) -&gt; None:
        """Test sending a message through the chat interface."""
        message = "Hello, how are you?"
        expected_response = "I'm doing well, thank you!"
    
        # Mock the OpenAI client response
        mock_response = AsyncMock()
        mock_response.choices = [AsyncMock(message=AsyncMock(content=expected_response))]
    
        mock_completions = AsyncMock()
        mock_completions.create = AsyncMock(return_value=mock_response)
    
        mock_chat = MagicMock()
        mock_chat.completions = mock_completions
    
        mock_client = AsyncMock()
        mock_client.chat = mock_chat
    
        with patch("openai.AsyncOpenAI", return_value=mock_client):
&gt;           response = await chat_interface.send_message(message)

tests\test_chat_interface.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\streamlitchat\chat_interface.py:111: in send_message
    response = await client.chat.completions.create(
.venv\Lib\site-packages\openai\resources\chat\completions.py:1661: in create
    return await self._post(
.venv\Lib\site-packages\openai\_base_client.py:1839: in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
.venv\Lib\site-packages\openai\_base_client.py:1533: in request
    return await self._request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;openai.AsyncOpenAI object at 0x000001FF59DCB500&gt;
cast_to = &lt;class 'openai.types.chat.chat_completion.ChatCompletion'&gt;
options = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeo...son_data={'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'model': 'gpt-3.5-turbo'}, extra_json=None)

    async def _request(
        self,
        cast_to: Type[ResponseT],
        options: FinalRequestOptions,
        *,
        stream: bool,
        stream_cls: type[_AsyncStreamT] | None,
        retries_taken: int,
    ) -&gt; ResponseT | _AsyncStreamT:
        if self._platform is None:
            # `get_platform` can make blocking IO calls so we
            # execute it earlier while we are in an async context
            self._platform = await asyncify(get_platform)()
    
        # create a copy of the options we were given so that if the
        # options are mutated later &amp; we then retry, the retries are
        # given the original options
        input_options = model_copy(options)
    
        cast_to = self._maybe_override_cast_to(cast_to, options)
        options = await self._prepare_options(options)
    
        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken
        request = self._build_request(options, retries_taken=retries_taken)
        await self._prepare_request(request)
    
        kwargs: HttpxSendArgs = {}
        if self.custom_auth is not None:
            kwargs["auth"] = self.custom_auth
    
        try:
            response = await self._client.send(
                request,
                stream=stream or self._should_stream_response_body(request=request),
                **kwargs,
            )
        except httpx.TimeoutException as err:
            log.debug("Encountered httpx.TimeoutException", exc_info=True)
    
            if remaining_retries &gt; 0:
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    stream=stream,
                    stream_cls=stream_cls,
                    response_headers=None,
                )
    
            log.debug("Raising timeout error")
            raise APITimeoutError(request=request) from err
        except Exception as err:
            log.debug("Encountered Exception", exc_info=True)
    
            if remaining_retries &gt; 0:
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    stream=stream,
                    stream_cls=stream_cls,
                    response_headers=None,
                )
    
            log.debug("Raising connection error")
            raise APIConnectionError(request=request) from err
    
        log.debug(
            'HTTP Request: %s %s "%i %s"', request.method, request.url, response.status_code, response.reason_phrase
        )
    
        try:
            response.raise_for_status()
        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code
            log.debug("Encountered httpx.HTTPStatusError", exc_info=True)
    
            if remaining_retries &gt; 0 and self._should_retry(err.response):
                await err.response.aclose()
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    response_headers=err.response.headers,
                    stream=stream,
                    stream_cls=stream_cls,
                )
    
            # If the response is streamed then we need to explicitly read the response
            # to completion before attempting to access the response text.
            if not err.response.is_closed:
                await err.response.aread()
    
            log.debug("Re-raising status error")
&gt;           raise self._make_status_error_from_response(err.response) from None
E           openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: test-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

.venv\Lib\site-packages\openai\_base_client.py:1634: AuthenticationError</failure></testcase><testcase classname="tests.test_chat_interface" name="test_streaming_response" time="0.336"><failure message="openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: test-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}">chat_interface = &lt;streamlitchat.chat_interface.ChatInterface object at 0x000001FF59D86930&gt;

    @pytest.mark.asyncio
    async def test_streaming_response(chat_interface: ChatInterface) -&gt; None:
        """Test streaming response from the chat interface."""
        message = "Hello, how are you?"
        expected_chunks = ["I'm", " doing", " well", ", thank", " you!"]
    
        # Mock streaming response chunks
        async def mock_stream():
            for chunk in expected_chunks:
                yield AsyncMock(choices=[AsyncMock(delta=AsyncMock(content=chunk))])
    
        mock_completions = AsyncMock()
        mock_completions.create = AsyncMock(return_value=mock_stream())
    
        mock_chat = MagicMock()
        mock_chat.completions = mock_completions
    
        mock_client = AsyncMock()
        mock_client.chat = mock_chat
    
        with patch("openai.AsyncOpenAI", return_value=mock_client):
            chunks = []
&gt;           async for chunk in chat_interface.send_message_stream(message):

tests\test_chat_interface.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\streamlitchat\chat_interface.py:160: in send_message_stream
    stream = await client.chat.completions.create(
.venv\Lib\site-packages\openai\resources\chat\completions.py:1661: in create
    return await self._post(
.venv\Lib\site-packages\openai\_base_client.py:1839: in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
.venv\Lib\site-packages\openai\_base_client.py:1533: in request
    return await self._request(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;openai.AsyncOpenAI object at 0x000001FF59FAB740&gt;
cast_to = &lt;class 'openai.types.chat.chat_completion.ChatCompletion'&gt;
options = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeo...ges': [{'role': 'user', 'content': 'Hello, how are you?'}], 'model': 'gpt-3.5-turbo', 'stream': True}, extra_json=None)

    async def _request(
        self,
        cast_to: Type[ResponseT],
        options: FinalRequestOptions,
        *,
        stream: bool,
        stream_cls: type[_AsyncStreamT] | None,
        retries_taken: int,
    ) -&gt; ResponseT | _AsyncStreamT:
        if self._platform is None:
            # `get_platform` can make blocking IO calls so we
            # execute it earlier while we are in an async context
            self._platform = await asyncify(get_platform)()
    
        # create a copy of the options we were given so that if the
        # options are mutated later &amp; we then retry, the retries are
        # given the original options
        input_options = model_copy(options)
    
        cast_to = self._maybe_override_cast_to(cast_to, options)
        options = await self._prepare_options(options)
    
        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken
        request = self._build_request(options, retries_taken=retries_taken)
        await self._prepare_request(request)
    
        kwargs: HttpxSendArgs = {}
        if self.custom_auth is not None:
            kwargs["auth"] = self.custom_auth
    
        try:
            response = await self._client.send(
                request,
                stream=stream or self._should_stream_response_body(request=request),
                **kwargs,
            )
        except httpx.TimeoutException as err:
            log.debug("Encountered httpx.TimeoutException", exc_info=True)
    
            if remaining_retries &gt; 0:
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    stream=stream,
                    stream_cls=stream_cls,
                    response_headers=None,
                )
    
            log.debug("Raising timeout error")
            raise APITimeoutError(request=request) from err
        except Exception as err:
            log.debug("Encountered Exception", exc_info=True)
    
            if remaining_retries &gt; 0:
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    stream=stream,
                    stream_cls=stream_cls,
                    response_headers=None,
                )
    
            log.debug("Raising connection error")
            raise APIConnectionError(request=request) from err
    
        log.debug(
            'HTTP Request: %s %s "%i %s"', request.method, request.url, response.status_code, response.reason_phrase
        )
    
        try:
            response.raise_for_status()
        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code
            log.debug("Encountered httpx.HTTPStatusError", exc_info=True)
    
            if remaining_retries &gt; 0 and self._should_retry(err.response):
                await err.response.aclose()
                return await self._retry_request(
                    input_options,
                    cast_to,
                    retries_taken=retries_taken,
                    response_headers=err.response.headers,
                    stream=stream,
                    stream_cls=stream_cls,
                )
    
            # If the response is streamed then we need to explicitly read the response
            # to completion before attempting to access the response text.
            if not err.response.is_closed:
                await err.response.aread()
    
            log.debug("Re-raising status error")
&gt;           raise self._make_status_error_from_response(err.response) from None
E           openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: test-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

.venv\Lib\site-packages\openai\_base_client.py:1634: AuthenticationError</failure></testcase><testcase classname="tests.test_chat_interface" name="test_clear_chat_history" time="0.001" /><testcase classname="tests.test_chat_interface" name="test_export_chat_history" time="0.001" /><testcase classname="tests.test_chat_interface" name="test_import_chat_history" time="0.001" /><testcase classname="tests.test_chat_interface" name="test_validate_api_key" time="0.001" /><testcase classname="tests.test_streamlitchat" name="test_content" time="0.000" /></testsuite></testsuites>